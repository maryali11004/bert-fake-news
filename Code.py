# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ODcpKEyB-zxSlzvvkUBk-vHAIAxwgFP
"""

!pip install transformers datasets torch scikit-learn pandas

# Full Code for Preprocessing and Feature Selection Before Feeding Data to BERT

import pandas as pd
import re
import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.optim import lr_scheduler
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
import nltk

# Download NLTK stopwords
nltk.download('stopwords')

# Load Data
train_data = pd.read_csv('train.tsv', sep='\t', header=None)
test_data = pd.read_csv('test.tsv', sep='\t', header=None)
valid_data = pd.read_csv('valid.tsv', sep='\t', header=None)

# Map multi-class labels to binary classification
def map_labels(label):
    return 1 if label in ['true', 'mostly-true'] else 0

train_data['binary_label'] = train_data[1].apply(map_labels)
test_data['binary_label'] = test_data[1].apply(map_labels)
valid_data['binary_label'] = valid_data[1].apply(map_labels)

# Preprocessing function to clean text
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

# Apply preprocessing
train_data[2] = train_data[2].apply(preprocess_text)
test_data[2] = test_data[2].apply(preprocess_text)
valid_data[2] = valid_data[2].apply(preprocess_text)

# Feature selection: Extract the most common words (for analysis purposes)
vectorizer = CountVectorizer(max_features=500)
vectorizer.fit(train_data[2])

# Tokenize Data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_data(texts, labels, tokenizer, max_length=128):
    encoding = tokenizer(
        list(texts),
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )
    return encoding['input_ids'], encoding['attention_mask'], labels

train_inputs, train_masks, train_labels = tokenize_data(train_data[2], train_data['binary_label'], tokenizer)
test_inputs, test_masks, test_labels = tokenize_data(test_data[2], test_data['binary_label'], tokenizer)
valid_inputs, valid_masks, valid_labels = tokenize_data(valid_data[2], valid_data['binary_label'], tokenizer)

# Create DataLoader
def create_dataloader(inputs, masks, labels, batch_size=16):
    dataset = TensorDataset(inputs, masks, torch.tensor(labels.values))
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_dataloader = create_dataloader(train_inputs, train_masks, train_labels)
test_dataloader = create_dataloader(test_inputs, test_masks, test_labels)
valid_dataloader = create_dataloader(valid_inputs, valid_masks, valid_labels)

# Load Pre-trained BERT Model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Optimizer and Scheduler
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Training Loop
epochs = 7
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch + 1}"):
        batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]

        optimizer.zero_grad()
        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks, labels=batch_labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}: Loss = {total_loss / len(train_dataloader)}")
    scheduler.step()

# Evaluation Function
def evaluate_model(model, dataloader):
    model.eval()
    predictions, true_labels = [], []

    with torch.no_grad():
        for batch in dataloader:
            batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]

            outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)
            logits = outputs.logits
            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())
            true_labels.extend(batch_labels.cpu().numpy())

    acc = accuracy_score(true_labels, predictions)
    report = classification_report(true_labels, predictions)
    return acc, report

# Evaluate on Validation and Test Sets
valid_acc, valid_report = evaluate_model(model, valid_dataloader)
print(f"Validation Accuracy: {valid_acc}")
print(valid_report)

test_acc, test_report = evaluate_model(model, test_dataloader)
print(f"Test Accuracy: {test_acc}")
print(test_report)

# Save the Model
model.save_pretrained('bert-fake-news')
tokenizer.save_pretrained('bert-fake-news')

# Load the Model for Inference
loaded_tokenizer = BertTokenizer.from_pretrained('bert-fake-news')
loaded_model = BertForSequenceClassification.from_pretrained('bert-fake-news')
loaded_model.to(device)

# Inference Example
def predict_statement(statement):
    loaded_model.eval()
    inputs = loaded_tokenizer(
        statement, return_tensors='pt', padding=True, truncation=True, max_length=128
    )
    inputs = {key: val.to(device) for key, val in inputs.items()}
    with torch.no_grad():
        logits = loaded_model(**inputs).logits
        prediction = torch.argmax(logits, dim=1).item()
    return "True" if prediction == 1 else "Not True"

# Example Usage
print(predict_statement("The government will reduce taxes next year."))